from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from sqlalchemy import select, func
from datetime import datetime
from typing import Optional
import csv
import io
import os
import time
import re

from app.db import get_db
from app.models import ExpenseDataset, ExpenseRow
from app.auth import basic_auth  # ğŸ” Basicèªè¨¼
from app.utils.csv_validator import validate_csv  # ï¼ˆä»»æ„ã€ä»Šå¾Œã®æ‹¡å¼µç”¨ï¼‰
from app.logger import logger

router = APIRouter(tags=["expenses"])

# =====================================================
# ğŸ§© ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =====================================================

MAX_SIZE = 10 * 1024 * 1024  # 10MBä¸Šé™

def ensure_uploads_dir():
    os.makedirs("uploads", exist_ok=True)
    return "uploads"

def sanitize_filename(name: str) -> str:
    return re.sub(r'[\\/*?:"<>|]+', "_", name)

def timestamp_prefix() -> str:
    return time.strftime("%Y%m%d_%H%M%S")

def validate_file_extension(file: UploadFile):
    """æ‹¡å¼µå­ãŒ .csv ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯"""
    filename = file.filename or ""
    if not filename.lower().endswith(".csv"):
        raise HTTPException(
            status_code=400,
            detail="CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.csvï¼‰ã®ã¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™"
        )

def validate_file_size(file: UploadFile):
    """10MBè¶…éãƒã‚§ãƒƒã‚¯"""
    file.file.seek(0, 2)
    size = file.file.tell()
    file.file.seek(0)
    if size > MAX_SIZE:
        raise HTTPException(
            status_code=413,
            detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ10MBã‚’è¶…ãˆã¦ã„ã¾ã™"
        )

def read_csv(file: UploadFile):
    """CSVå†…å®¹ã®æ¤œè¨¼"""
    validate_file_extension(file)
    validate_file_size(file)

    try:
        content = file.file.read().decode("utf-8-sig")
    except Exception:
        raise HTTPException(status_code=400, detail="ãƒ•ã‚¡ã‚¤ãƒ«ã‚’UTF-8ã¨ã—ã¦èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚")

    reader = csv.reader(io.StringIO(content))
    rows = list(reader)
    if not rows:
        raise HTTPException(status_code=400, detail="CSVãŒç©ºã§ã™ã€‚")

    header = rows[0]
    if len(set(header)) != len(header):
        raise HTTPException(status_code=400, detail="ãƒ˜ãƒƒãƒ€ã«é‡è¤‡ãŒã‚ã‚Šã¾ã™ã€‚")

    data_rows = rows[1:]
    for i, r in enumerate(data_rows, start=2):
        if len(r) != len(header):
            raise HTTPException(status_code=400, detail=f"{i}è¡Œç›®ã®åˆ—æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚")

    return header, [dict(zip(header, row)) for row in data_rows]


# =====================================================
# â‘  CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =====================================================
@router.post("/", status_code=201)
def upload_expense(
    file: UploadFile = File(...),
    branch_name: str = Form(..., description="æ”¯åº—åï¼ˆä¾‹ï¼šå¤§é˜ªæ”¯åº—ï¼‰"),
    period: str = Form(..., description="æå‡ºæœˆï¼ˆYYYY-MMå½¢å¼ ä¾‹ï¼š2025-10ï¼‰"),
    db: Session = Depends(get_db),
    user: str = Depends(basic_auth),
):
    # ğŸ“… æœŸé–“ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    if not re.match(r"^\d{4}-(0[1-9]|1[0-2])$", period):
        raise HTTPException(
            status_code=400,
            detail="periodã¯YYYY-MMå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        )

    # ğŸ“„ CSVãƒã‚§ãƒƒã‚¯ï¼ˆæ‹¡å¼µå­ãƒ»ã‚µã‚¤ã‚ºãƒ»æ§‹é€ ï¼‰
    header, rows = read_csv(file)

    # ğŸ—‚ï¸ ä¿å­˜å‡¦ç†
    uploads_dir = ensure_uploads_dir()
    safe_name = sanitize_filename(file.filename)
    save_name = f"{timestamp_prefix()}_{safe_name}"
    save_path = os.path.join(uploads_dir, save_name)

    with open(save_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        for r in rows:
            writer.writerow(r.values())

    # ğŸ“ DBç™»éŒ²
    dataset = ExpenseDataset(
        file_name=safe_name,
        row_count=len(rows),
        original_path=save_path,
        branch_name=branch_name,
        period=period,
    )
    db.add(dataset)
    db.flush()
    db.bulk_save_objects([ExpenseRow(dataset_id=dataset.id, row_data=r) for r in rows])
    db.commit()
    db.refresh(dataset)

    logger.info(f"[UPLOAD SUCCESS] user={user}, file={safe_name}, branch={branch_name}, period={period}, rows={len(rows)}")

    return {
        "status": "success",
        "dataset_id": dataset.id,
        "branch_name": branch_name,
        "period": period,
        "uploaded_at": str(dataset.uploaded_at),
        "row_count": len(rows),
        "file": safe_name,
        "saved_path": save_path,
    }


# =====================================================
# â‘¡ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´ä¸€è¦§
# =====================================================
@router.get("/")
def list_datasets(
    branch: Optional[str] = Query(None),
    period: Optional[str] = Query(None),
    db: Session = Depends(get_db),
    user: str = Depends(basic_auth),
):
    stmt = select(ExpenseDataset).order_by(ExpenseDataset.uploaded_at.desc())
    if branch:
        stmt = stmt.where(ExpenseDataset.branch_name == branch)
    if period:
        stmt = stmt.where(ExpenseDataset.period == period)

    datasets = db.execute(stmt).scalars().all()
    return {
        "data": [
            {
                "id": d.id,
                "file_name": d.file_name,
                "row_count": d.row_count,
                "uploaded_at": str(d.uploaded_at),
                "branch_name": d.branch_name,
                "period": d.period,
            }
            for d in datasets
        ]
    }


@router.get("", include_in_schema=False)
def list_datasets_no_trailing_slash(
    branch: Optional[str] = Query(None),
    period: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    return list_datasets(branch=branch, period=period, db=db)


# =====================================================
# â‘¢ æ¨ªæ–­JSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
# =====================================================
@router.get("/download_all_json")
def download_all_json(
    branch_name: Optional[str] = Query(None),
    branch: Optional[str] = Query(None, alias="branch"),
    period: Optional[str] = Query(None),
    filter_col: Optional[str] = Query(None),
    filter_val: Optional[str] = Query(None),
    page: int = Query(1, ge=1),
    size: int = Query(50, ge=1, le=1000),
    db: Session = Depends(get_db),
    user: str = Depends(basic_auth),
):
    target_branch = branch_name or branch
    stmt = select(ExpenseRow.row_data).join(ExpenseDataset, ExpenseRow.dataset_id == ExpenseDataset.id)

    if target_branch:
        stmt = stmt.where(ExpenseDataset.branch_name == target_branch)
    if period:
        stmt = stmt.where(ExpenseDataset.period == period)
    if filter_col and filter_val:
        stmt = stmt.where(ExpenseRow.row_data[filter_col].astext.ilike(f"%{filter_val}%"))

    total = db.scalar(select(func.count()).select_from(stmt.subquery()))
    offset = (page - 1) * size
    rows = db.execute(stmt.offset(offset).limit(size)).all()

    return {"meta": {"total": total, "page": page, "size": size}, "data": [r[0] for r in rows]}


# =====================================================
# â‘£ æ¨ªæ–­CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =====================================================
@router.get("/download_all_csv")
def download_all_csv(
    branch_name: Optional[str] = Query(None),
    branch: Optional[str] = Query(None, alias="branch"),
    period: Optional[str] = Query(None),
    filter_col: Optional[str] = Query(None),
    filter_val: Optional[str] = Query(None),
    db: Session = Depends(get_db),
    user: str = Depends(basic_auth),
):
    target_branch = branch_name or branch
    stmt = select(ExpenseRow.row_data).join(ExpenseDataset, ExpenseRow.dataset_id == ExpenseDataset.id)

    if target_branch:
        stmt = stmt.where(ExpenseDataset.branch_name == target_branch)
    if period:
        stmt = stmt.where(ExpenseDataset.period == period)
    if filter_col and filter_val:
        stmt = stmt.where(ExpenseRow.row_data[filter_col].astext.ilike(f"%{filter_val}%"))

    def generate():
        rows = db.execute(stmt).all()
        if not rows:
            yield ""
            return
        header = list(rows[0][0].keys())
        sio = io.StringIO()
        writer = csv.writer(sio, lineterminator="\n")
        writer.writerow(header)
        for r in rows:
            writer.writerow([r[0].get(h, "") for h in header])
        yield sio.getvalue()

    filename = f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    return StreamingResponse(
        generate(),
        media_type="text/csv",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


# =====================================================
# â‘¤ ç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ˜ç´°
# =====================================================
@router.get("/{dataset_id}")
def get_dataset_details(
    dataset_id: int,
    page: int = Query(1, ge=1),
    size: int = Query(20, ge=1, le=200),
    filter_col: Optional[str] = Query(None),
    filter_val: Optional[str] = Query(None),
    db: Session = Depends(get_db),
    user: str = Depends(basic_auth),
):
    dataset = db.get(ExpenseDataset, dataset_id)
    if not dataset:
        raise HTTPException(status_code=404, detail="æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    stmt = select(ExpenseRow).where(ExpenseRow.dataset_id == dataset_id)
    if filter_col and filter_val:
        stmt = stmt.where(ExpenseRow.row_data[filter_col].astext.ilike(f"%{filter_val}%"))

    total = db.scalar(select(func.count()).select_from(stmt.subquery()))
    offset = (page - 1) * size
    rows = db.execute(stmt.offset(offset).limit(size)).scalars().all()

    return {
        "meta": {
            "branch_name": dataset.branch_name,
            "period": dataset.period,
            "total": total,
            "page": page,
            "size": size,
        },
        "data": [r.row_data for r in rows],
    }


# =====================================================
# â‘¥ ãƒ‡ãƒãƒƒã‚°ç”¨API
# =====================================================
@router.get("/_debug/dbinfo")
def debug_info(
    db: Session = Depends(get_db),
    user: str = Depends(basic_auth),
):
    total = db.scalar(select(func.count(ExpenseDataset.id)))
    by_branch = db.execute(
        select(ExpenseDataset.branch_name, func.count())
        .group_by(ExpenseDataset.branch_name)
    ).all()
    return {
        "database": "expenses",
        "datasets": total,
        "by_branch": {b or "æœªè¨­å®š": c for b, c in by_branch},
    }
