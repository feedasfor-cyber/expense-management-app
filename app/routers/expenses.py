import os
import io
import csv
from datetime import datetime
from fastapi import APIRouter, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from psycopg2.extras import Json
from app.database import get_connection
from app.utils.csv_validator import validate_csv

router = APIRouter()

# =====================================================
# å®šæ•°ãƒ»åˆæœŸè¨­å®š
# =====================================================
UPLOAD_DIR = "uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)

# =====================================================
# å…±é€šï¼šãƒ•ã‚£ãƒ«ã‚¿SQLæ§‹ç¯‰é–¢æ•°
# =====================================================
def build_filter_conditions(
    filter_col=None, filter_val=None,
    num_col=None, num_op=None, num_val=None,
    date_col=None, date_from=None, date_to=None,
    include_dataset_id: bool = False
):
    """WHEREå¥ï¼‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    where_conditions = []
    params = []

    # æ–‡å­—åˆ—ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆORï¼‰
    if filter_col and filter_val:
        for col, val in zip(filter_col, filter_val):
            or_vals = [v.strip() for v in val.split(",") if v.strip()]
            conds = []
            for v in or_vals:
                conds.append("row_data ->> %s ILIKE %s")
                params.extend([col, f"%{v}%"])
            if conds:
                where_conditions.append("(" + " OR ".join(conds) + ")")

    # æ•°å€¤ãƒ•ã‚£ãƒ«ã‚¿
    if num_col and num_op and num_val:
        ops = {"gt": ">", "lt": "<", "ge": ">=", "le": "<=", "eq": "="}
        for col, op, val in zip(num_col, num_op, num_val):
            if op.lower() == "between":
                min_v, max_v = map(float, val.split(","))
                where_conditions.append("(CAST(row_data ->> %s AS NUMERIC) BETWEEN %s AND %s)")
                params.extend([col, min_v, max_v])
            elif op.lower() in ops:
                where_conditions.append(f"(CAST(row_data ->> %s AS NUMERIC) {ops[op.lower()]} %s)")
                params.extend([col, float(val)])
            else:
                raise HTTPException(status_code=400, detail=f"ç„¡åŠ¹ãªæ¯”è¼ƒæ¼”ç®—å­: {op}")

    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿
    if date_col:
        for col, start, end in zip(
            date_col,
            (date_from or []) + [None] * (len(date_col) - len(date_from or [])),
            (date_to or []) + [None] * (len(date_col) - len(date_to or [])),
        ):
            if start and end:
                where_conditions.append("(CAST(row_data ->> %s AS DATE) BETWEEN %s AND %s)")
                params.extend([col, start, end])
            elif start:
                where_conditions.append("(CAST(row_data ->> %s AS DATE) >= %s)")
                params.extend([col, start])
            elif end:
                where_conditions.append("(CAST(row_data ->> %s AS DATE) <= %s)")
                params.extend([col, end])

    where_clause = " AND ".join(where_conditions)
    if where_clause:
        where_clause = ("AND " if include_dataset_id else "WHERE ") + where_clause

    return where_clause, params


# =====================================================
# â‘  å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¨ªæ–­ï¼ˆCSVï¼‰
# =====================================================
@router.get("/download_all_csv")
def download_all_filtered_csv(
    filter_col: list[str] = Query(None),
    filter_val: list[str] = Query(None),
    num_col: list[str] = Query(None),
    num_op: list[str] = Query(None),
    num_val: list[str] = Query(None),
    date_col: list[str] = Query(None),
    date_from: list[str] = Query(None),
    date_to: list[str] = Query(None),
):
    conn = get_connection(); cur = conn.cursor()
    try:
        where_clause, params = build_filter_conditions(
            filter_col, filter_val, num_col, num_op, num_val,
            date_col, date_from, date_to
        )
        cur.execute(f"SELECT row_data FROM expense_rows {where_clause} ORDER BY dataset_id, id;", params)
        rows = [r[0] for r in cur.fetchall()]
        if not rows:
            raise HTTPException(status_code=404, detail="è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—")

        buf = io.StringIO()
        writer = csv.DictWriter(buf, fieldnames=list(rows[0].keys()))
        writer.writeheader(); writer.writerows(rows)
        buf.seek(0)
        return StreamingResponse(
            iter([buf.getvalue().encode("utf-8-sig")]),
            media_type="text/csv",
            headers={"Content-Disposition": 'attachment; filename="all_filtered_expenses.csv"'}
        )
    finally:
        cur.close(); conn.close()


# =====================================================
# â‘¡ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¨ªæ–­ï¼ˆJSONãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
# =====================================================
@router.get("/download_all_json")
def download_all_filtered_json(
    filter_col: list[str] = Query(None),
    filter_val: list[str] = Query(None),
    num_col: list[str] = Query(None),
    num_op: list[str] = Query(None),
    num_val: list[str] = Query(None),
    date_col: list[str] = Query(None),
    date_from: list[str] = Query(None),
    date_to: list[str] = Query(None),
    page: int = Query(1, ge=1),
    size: int = Query(50, ge=1, le=500),
):
    conn = get_connection(); cur = conn.cursor()
    try:
        where_clause, params = build_filter_conditions(
            filter_col, filter_val, num_col, num_op, num_val,
            date_col, date_from, date_to
        )
        cur.execute(f"SELECT COUNT(*) FROM expense_rows {where_clause};", params)
        total = cur.fetchone()[0]
        offset = (page - 1) * size
        cur.execute(
            f"SELECT row_data FROM expense_rows {where_clause} ORDER BY dataset_id, id LIMIT %s OFFSET %s;",
            params + [size, offset]
        )
        rows = [r[0] for r in cur.fetchall()]
        return {"meta": {"total": total, "page": page, "size": size}, "data": rows}
    finally:
        cur.close(); conn.close()


# =====================================================
# â‘¢ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =====================================================
@router.post("/")
async def upload_expense_csv(file: UploadFile = File(...)):
    print("==== [UPLOAD START] ====")
    print(f"ðŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«å: {file.filename}")

    raw = await file.read()
    if not raw:
        raise HTTPException(status_code=400, detail="ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã§ã™")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(UPLOAD_DIR, f"{timestamp}_{file.filename}")
    with open(save_path, "wb") as f:
        f.write(raw)
    print(f"âœ… ä¿å­˜: {save_path}")

    pseudo = io.BytesIO(raw)
    headers, rows = validate_csv(pseudo)
    print(f"âœ… CSVæ¤œè¨¼å®Œäº†: {len(rows)}è¡Œ")

    conn = get_connection(); cur = conn.cursor()
    try:
        cur.execute("""
            INSERT INTO expense_datasets (file_name, row_count, uploader, original_path)
            VALUES (%s, %s, %s, %s)
            RETURNING id, uploaded_at;
        """, (file.filename, len(rows), "admin", save_path))
        dataset_id, uploaded_at = cur.fetchone()

        for row in rows:
            cur.execute("INSERT INTO expense_rows (dataset_id, row_data) VALUES (%s, %s);",
                        (dataset_id, Json(dict(zip(headers, row)))))

        conn.commit()
        print(f"âœ… ç™»éŒ²å®Œäº†: dataset_id={dataset_id}")

        return {
            "status": "success",
            "dataset_id": dataset_id,
            "uploaded_at": uploaded_at.strftime("%Y-%m-%d %H:%M:%S") if uploaded_at else None,
            "row_count": len(rows),
            "file": file.filename,
            "saved_path": save_path
        }
    except Exception as e:
        conn.rollback()
        print("âŒ DBã‚¨ãƒ©ãƒ¼:", e)
        raise HTTPException(status_code=500, detail=f"DBã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        cur.close(); conn.close()
        print("==== [UPLOAD END] ====")


# =====================================================
# â‘£ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å±¥æ­´
# =====================================================
@router.get("/")
def list_expense_datasets():
    conn = get_connection(); cur = conn.cursor()
    try:
        cur.execute("""
            SELECT id, file_name, row_count, uploaded_at
            FROM expense_datasets
            ORDER BY uploaded_at DESC;
        """)
        rows = cur.fetchall()
        result = [
            {
                "id": r[0],
                "file_name": r[1],
                "row_count": r[2],
                "uploaded_at": r[3].strftime("%Y-%m-%d %H:%M:%S") if r[3] else None
            }
            for r in rows
        ]
        print(f"[HISTORY] {len(result)} datasets loaded")
        return JSONResponse(content=result)
    finally:
        cur.close(); conn.close()

# =====================================================
# â‘¤ æ˜Žç´°å–å¾—ï¼ˆç‰¹å®šãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ï¼‰
# =====================================================
@router.get("/{dataset_id}")
def get_expense_details(
    dataset_id: int,
    page: int = Query(1, ge=1),
    size: int = Query(20, ge=1, le=100),
):
    """æŒ‡å®šã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ˜Žç´°ã‚’è¿”ã™ï¼ˆãƒšãƒ¼ã‚¸ãƒ³ã‚°å¯¾å¿œï¼‰"""
    conn = get_connection()
    cur = conn.cursor()
    try:
        # å­˜åœ¨ç¢ºèª
        cur.execute("SELECT 1 FROM expense_datasets WHERE id=%s;", (dataset_id,))
        if not cur.fetchone():
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        # ç·ä»¶æ•°
        cur.execute("SELECT COUNT(*) FROM expense_rows WHERE dataset_id=%s;", (dataset_id,))
        total = cur.fetchone()[0]

        # ãƒšãƒ¼ã‚¸ãƒ³ã‚°å–å¾—
        offset = (page - 1) * size
        cur.execute("""
            SELECT row_data
            FROM expense_rows
            WHERE dataset_id=%s
            ORDER BY id
            LIMIT %s OFFSET %s;
        """, (dataset_id, size, offset))
        rows = [r[0] for r in cur.fetchall()]

        return {
            "meta": {"dataset_id": dataset_id, "total": total, "page": page, "size": size},
            "data": rows
        }
    finally:
        cur.close()
        conn.close()



# =====================================================
# â‘¤ å…ƒCSVå†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# =====================================================
@router.get("/{dataset_id}/download")
def download_expense_csv(dataset_id: int):
    conn = get_connection(); cur = conn.cursor()
    try:
        cur.execute("SELECT file_name, original_path FROM expense_datasets WHERE id=%s;", (dataset_id,))
        r = cur.fetchone()
        if not r:
            raise HTTPException(status_code=404, detail="Dataset not found")
        file_name, path = r
        if not os.path.exists(path):
            raise HTTPException(status_code=404, detail=f"File not found: {path}")
        return FileResponse(path, filename=file_name, media_type="text/csv")
    finally:
        cur.close(); conn.close()


# =====================================================
# â‘¥ ãƒ‡ãƒãƒƒã‚°APIï¼ˆDBæŽ¥ç¶šæƒ…å ±ï¼‹ä»¶æ•°ï¼‰
# =====================================================
@router.get("/_debug/dbinfo")
def debug_dbinfo():
    from urllib.parse import urlparse
    db_url = os.getenv("DATABASE_URL")
    parsed = urlparse(db_url)
    conn = get_connection(); cur = conn.cursor()
    try:
        cur.execute("SELECT COUNT(*) FROM expense_datasets;")
        count = cur.fetchone()[0]
    finally:
        cur.close(); conn.close()
    return {
        "db": parsed.path.lstrip("/"),
        "host": parsed.hostname,
        "port": parsed.port,
        "expense_datasets_count": count
    }
